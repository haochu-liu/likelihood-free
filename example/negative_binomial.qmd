---
title: "MCMC BSL with negative binomial example"
author: "Haochu"
format: html
editor: visual
---

```{r}
#| label: load-packages
#| include: false

library(mvtnorm)
library(rjags)
library(coda)
source("../BSL/SL_MCMC.R")
source("../BSL/SL_AM.R")
set.seed(100)
```

## Example

### Description

Suppose $y_1, \dots, y_{20}$ are independent observations from a negative binomial distribution NB($5, 0.5$). We take $s_{obs} = S(y_1, \dots, y_{20}) = \sum y$ as the observed statistics.

Assume the true distribution of $y_i$ is unknown, we model the $y_i$ as independent and coming from a Poisson($\theta$) distribution. So in our simulation, we simulate $z_1, \dots, z_{20}$ from Poisson($\theta$) and $s=\sum z$.

We set the prior for $\theta$ is Gamma($2, 0.5$).

In this toy example,
$$
\begin{align*}
y_1, \dots, y_{20} &\sim NB(5, 0.5),\\
s_{obs} = \sum_{i=1}^{20} y_i &\sim NB(100, 0.5),\\
\theta &\sim Gamma(2, 0.5),\\
z_1, \dots, z_{20} |\theta &\sim Poisson(\theta),\\
s |\theta= \sum_{i=1}^{20} z_i |\theta &\sim Poisson(20\theta).
\end{align*}
$$

### Code

```{r}
#| echo: true
n <- 20
obs <- rnbinom(1, size=5*n, prob=0.5)

iter <- 50000
init_theta <- c(rgamma(1, shape=2, rate=0.5))
prior_func <- function(theta){
  dgamma(theta, shape=2, rate=0.5, log=TRUE)
  }
sample_func <- function(theta, M) {
  s <- rpois(M, theta*n)
  return(list(mean=c(mean(s)),
              sigma=matrix(var(s), ncol=1, nrow=1)))
}
```

## MCMC BSL

### Find M

Find the value $M$ such that the variance of the log likelihood estimator is close to $3$.

```{r}
#| echo: true
M_seq <- seq(5, 100, by=5)
log_likelihood <- rep(NA, length(M_seq))
for (j in 1:length(M_seq)) {
  M <- M_seq[j]
  sl_vec <- rep(NA, 100)
  for (i in 1:100) {
    stats_M <- sample_func(init_theta, M)
    sl_vec[i] <- dmvnorm(x=obs,
                         mean=stats_M$mean,
                         sigma=stats_M$sigma,
                         log=TRUE)
  }
  log_likelihood[j] <- var(sl_vec)
}

print(log_likelihood)
M <- M_seq[5]
```

### MCMC BSL

Run `SL_MCMC` for a Markov chain with length $50000$.

```{r}
#| echo: true
theta_seq <- SL_MCMC(M, iter, obs, init_theta, prior_func, sample_func, 0.1,
                     acc_rate=TRUE)
```

### Plots

#### Trace plot

```{r}
#| echo: true
plot(1:iter, theta_seq$theta[1, ], type = "l", xlab="Iterations", ylab="Theta")
```

#### Density plot

We have $\pi(\theta) = Gamma(\theta; 2, 0.5)$ as the prior. From the assumption, the model is $p(s|\theta) = Poisson(s; 20\theta)$. The posterior is $\pi(\theta|s) = Gamma(\theta; 2+s, 20.5)$.

Since we know the true model $y_i \sim NB(5, 0.5)$, let us assume that the model is $NB(5, p)$ with $5(1-p)/p = \theta$ and true parameter $p^* = 0.5$. We can have the true model as $p^*(s|\theta) = NB(100, \frac{5}{\theta + 5})$ and $\theta^* = 5$. The posterior $\pi^*(\theta|s)$ can be simulated by MCMC.

```{r}
#| echo: true
prior_a <- 2
prior_b <- 0.5

jags_data <- list(
  y = obs,
  N = 1,
  r = 100,
  prior_a = prior_a,
  prior_b = prior_b
)

model_fit <- jags.model(
  file = "negative_binomial.txt",
  data = jags_data,
  n.chains = 3,
  n.adapt = 1000
)

posterior_star_coda <- coda.samples(
  model = model_fit,
  variable.names = "theta",
  n.iter = 10000,
  thin = 10
)

posterior_star <- as.data.frame(as.matrix(posterior_star_coda))
```

The density functions are plotted below:

```{r}
#| echo: true
theta_density <- density(theta_seq$theta[1, 10000:iter])
x_values <- seq(min(theta_seq$theta[1, 10000:iter]),
                max(theta_seq$theta[1, 10000:iter]),
                length.out = 1000)
y_values <- dgamma(x_values,
                   shape=(2 + obs),
                   rate=(0.5 + 20))
posterior_star_density <- density(posterior_star$theta)
y_prior_values <- dgamma(x_values,
                   shape=2,
                   rate=0.5)

plot(theta_density,
     main = "Density functions",
     xlab = "Theta",
     ylab = "Density",
     col = "black",
     lwd = 2)
lines(x_values, y_values, col = "blue", lwd = 2, lty = 2)
lines(posterior_star_density, col = "red", lwd = 2, lty = 4)
lines(x_values, y_prior_values, col = "gray", lwd = 2, lty = 1)
legend("topright",
       c("Synthetic posterior", "Exact posterior", "True posterior", "Prior"),
       col = c("black", "blue", "red", "gray"),
       lty = c(1, 2, 4, 1),
       lwd = 2)
```

### AM BSL

Use adaptive M-H algorithm to tune the proposal of M-H. Run `SL_AM` with a same setup as `SL_MCMC`.

```{r}
#| echo: true
theta_seq_AM <- SL_AM(M, iter, 5000, obs, init_theta, prior_func, sample_func,
                      0.1, 0.01, acc_rate=TRUE)
```

Its trace plot is

```{r}
#| echo: true
plot(1:iter, theta_seq_AM$theta[1, ], type = "l", xlab="Iterations", ylab="Theta")
```

Compare the effective sample sizes for both two methods:

```{r}
#| echo: true
print("M-H algorithm:")
print(paste0("Acceptance rate: ", theta_seq$acc_rate))
mcmc_chain <- as.mcmc(theta_seq$theta[1, ])
print(paste0("ESS: ", effectiveSize(mcmc_chain)))
print("Adaptive M-H algorithm:")
print(paste0("Acceptance rate: ", theta_seq_AM$acc_rate))
print(paste0("Acceptance rate after the burn-in period: ", theta_seq_AM$acc_rate2))
mcmc_chain_AM <- as.mcmc(theta_seq_AM$theta[1, ])
print(paste0("ESS: ", effectiveSize(mcmc_chain_AM)))
```

### Different M

#### Different M and parameter

We consider three different values, $M = 5, 20, 50$. And compare the variance of log-likelihood for different parameter values.

```{r}
#| echo: true
M_vec <- c(5, 20, 50)
theta_vec <- seq(2, 6, length.out = 401)

var_mat <- matrix(NA, nrow=401, ncol=3)
var_likelihood_mat <- matrix(NA, nrow=401, ncol=3)
for (i in 1:3) {
  M <- M_vec[i]
  for (j in 1:401) {
    theta <- theta_vec[j]
    sl_vec <- rep(NA, 100)
    sl_likelihood_Vec <- rep(NA, 100)
    for (k in 1:100) {
      stats_M <- sample_func(theta, M)
      sl_vec[k] <- dmvnorm(x=obs,
                           mean=stats_M$mean,
                           sigma=stats_M$sigma,
                           log=TRUE)
      sl_likelihood_Vec[k] <- dmvnorm(x=obs,
                                      mean=stats_M$mean,
                                      sigma=stats_M$sigma,
                                      log=FALSE)
    }
    var_mat[j, i] <- var(sl_vec)
    var_likelihood_mat[j, i] <- var(sl_likelihood_Vec)
  }
  print(paste0("Finish iterations for M = ", M))
}
```

```{r}
#| echo: true
f_values <- dgamma(theta_vec,
                   shape=(2 + obs),
                   rate=(0.5 + 20))

par(mar = c(5, 4, 4, 4) + 0.1)

plot(theta_vec, log(var_mat[, 1]),
     type = "l",
     col = "darkblue",
     xlab = "Theta",
     ylab = "log(Var(log-likelihood))",
     ylim = range(log(var_mat)),
     lwd = 2)
lines(theta_vec, log(var_mat[, 2]), col = "darkgreen", type = "l", lwd = 2)
lines(theta_vec, log(var_mat[, 3]), col = "darkred", type = "l", lwd = 2)
lines(theta_vec, rep(log(1), 401), col = "gray", type = "l", lwd = 2)
lines(theta_vec, rep(log(3), 401), col = "gray", type = "l", lwd = 2)

par(new = TRUE)

plot(theta_vec, f_values,
     type = "l",
     col = "black",
     xlab = "",
     ylab = "",
     ylim = range(f_values),
     axes = FALSE,
     lwd = 2)

axis(side = 4, col = "black", col.axis = "black")

mtext("Density", side = 4, line = 3, col = "black")

legend("topright",
       legend = c("M = 5", "M = 20", "M = 50", "Exact posterior"),
       col = c("darkblue", "darkgreen", "darkred", "black"),
       lty = 1,
       lwd = 2,
       bty = "n")
```

#### Different M for posterior

Let's choose different values of $M$ to see how it affects the posterior given by BSL. In the same example, we consider $M = 5, 10, 15, 20, 25, 30, 50, 100$. And plot their density curves.

```{r}
#| echo: true
M_df <- data.frame(values = c(5, 10, 15, 20, 25, 30, 50, 100),
                   acc = NA,
                   ESS = NA,
                   var = NA)
theta_matrix <- matrix(NA, nrow=8, ncol=50000)

for (j in 1:8) {
  M <- M_df$values[j]
  sl_vec <- rep(NA, 100)
  for (i in 1:100) {
    stats_M <- sample_func(init_theta, M)
    sl_vec[i] <- dmvnorm(x=obs,
                         mean=stats_M$mean,
                         sigma=stats_M$sigma,
                         log=TRUE)
  }
  M_df$var[j] <- var(sl_vec)
  chain <- SL_MCMC(M, iter, obs, init_theta, prior_func, sample_func, 0.1,
                   acc_rate=TRUE)
  M_df$acc[j] <- chain$acc_rate
  theta_matrix[j, ] <- chain$theta[1, ]
  mcmc_chain <- as.mcmc(chain$theta[1, ])
  M_df$ESS[j] <- effectiveSize(mcmc_chain)
  print(paste0("Finish iteration ", j))
}
```

```{r}
#| echo: true
print(M_df)
```

```{r}
#| echo: true
density1 <- density(theta_matrix[1, ])
plot(density1,
     main = "Posterior functions",
     xlab = "Theta",
     ylab = "Density",
     col = 1,
     lwd = 2)
for (i in 2:8) {
  density2 <- density(theta_matrix[i, ])
  lines(density2, col = i, lwd = 2)
}
lines(x_values, y_values, col = 9, lwd = 2, lty = 2)
legend("topright",
       legend = c(paste("M =", M_df$values), "Exact posterior"),
       col = 1:9,
       lwd = 2,
       cex = 0.8)
```

## SMC BSL



## Efficiency

```{r}
#| echo: true
efficiency_df <- as.data.frame(matrix(NA, nrow=3, ncol=5))
rownames(efficiency_df) <- c("BSL_MCMC", "BSL_AM", "BSL_SMC")
colnames(efficiency_df) <- c("time(sec)", "ESS/sec",
                             "V_QL*sec", "V_QU*sec", "V_E*sec")
```

Run 100 chains from BSL MCMC.

```{r}
#| echo: true
R = 100

time_vec <- rep(NA, R)
ess_vec <- rep(NA, R)
ql_vec <- rep(NA, R)
qu_vec <- rep(NA, R)
e_vec <- rep(NA, R)
for (r in 1:R) {
  time_result <- system.time(
  chain_r <- SL_MCMC(M, iter, obs, init_theta, prior_func, sample_func,
                     0.1)
  )
  time_vec[r] <- time_result["elapsed"]
  mcmc_chain <- as.mcmc(chain_r[1, (iter-1000):iter])
  ess_vec[r] <- effectiveSize(mcmc_chain)
  ql_vec[r] <- quantile(chain_r[1, (iter-1000):iter], probs=0.25)
  qu_vec[r] <- quantile(chain_r[1, (iter-1000):iter], probs=0.75)
  e_vec[r] <- mean(chain_r[1, (iter-1000):iter])
  
  if (r %% 10 == 0) {print(paste0("Finish iteration ", r))}
}
efficiency_df$`time(sec)`[1] <- mean(time_vec)
efficiency_df$`ESS/sec`[1] <- mean(ess_vec / time_vec)
efficiency_df$`V_QL*sec`[1] <- var(ql_vec) * mean(time_vec)
efficiency_df$`V_QU*sec`[1] <- var(qu_vec) * mean(time_vec)
efficiency_df$`V_E*sec`[1] <- var(e_vec) * mean(time_vec)
```

Run 100 chains from BSL AM.

```{r}
#| echo: true
time_vec <- rep(NA, R)
ess_vec <- rep(NA, R)
ql_vec <- rep(NA, R)
qu_vec <- rep(NA, R)
e_vec <- rep(NA, R)
for (r in 1:R) {
  time_result <- system.time(
  chain_r <- SL_AM(M, iter, 5000, obs, init_theta, prior_func, sample_func,
                   0.1, 0.01)
  )
  time_vec[r] <- time_result["elapsed"]
  mcmc_chain <- as.mcmc(chain_r[1, (iter-1000):iter])
  ess_vec[r] <- effectiveSize(mcmc_chain)
  ql_vec[r] <- quantile(chain_r[1, (iter-1000):iter], probs=0.25)
  qu_vec[r] <- quantile(chain_r[1, (iter-1000):iter], probs=0.75)
  e_vec[r] <- mean(chain_r[1, (iter-1000):iter])
  
  if (r %% 10 == 0) {print(paste0("Finish iteration ", r))}
}
efficiency_df$`time(sec)`[2] <- mean(time_vec)
efficiency_df$`ESS/sec`[2] <- mean(ess_vec / time_vec)
efficiency_df$`V_QL*sec`[2] <- var(ql_vec) * mean(time_vec)
efficiency_df$`V_QU*sec`[2] <- var(qu_vec) * mean(time_vec)
efficiency_df$`V_E*sec`[2] <- var(e_vec) * mean(time_vec)
```



